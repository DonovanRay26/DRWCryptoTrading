{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28a63c63",
   "metadata": {},
   "source": [
    "## Ensemble Approach\n",
    "\n",
    "### Models Used: \n",
    "#### - Trees: LightGBM, CatBoost\n",
    "#### - Neural Network: TabNet\n",
    "#### - Time Series: N-HiTS\n",
    "#### - Linear: Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "973a6bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# Imports\n",
    "# ==============================\n",
    "\n",
    "import pandas as pd, numpy as np, matplotlib.pyplot as plt, seaborn as sns\n",
    "import lightgbm as lgb\n",
    "import catboost as cb\n",
    "from sklearn.linear_model import Ridge\n",
    "from pytorch_tabnet.tab_model import TabNetRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb37df7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method NDFrame.head of                      bid_qty  ask_qty  buy_qty  sell_qty   volume        X1  \\\n",
      "2023-03-01 00:00:00   15.283    8.425  176.405    44.984  221.389  0.181844   \n",
      "2023-03-01 00:01:00   38.590    2.336  525.846   321.950  847.796  0.489497   \n",
      "2023-03-01 00:02:00    0.442   60.250  159.227   136.369  295.596  0.260121   \n",
      "2023-03-01 00:03:00    4.865   21.016  335.742   124.963  460.705  0.099976   \n",
      "2023-03-01 00:04:00   27.158    3.451   98.411    44.407  142.818  0.270893   \n",
      "...                      ...      ...      ...       ...      ...       ...   \n",
      "2024-02-29 23:55:00    4.163    6.805   39.037    55.351   94.388  0.010535   \n",
      "2024-02-29 23:56:00    2.290    4.058  110.201    67.171  177.372  0.003939   \n",
      "2024-02-29 23:57:00    5.237    3.640   70.499    30.753  101.252  0.053320   \n",
      "2024-02-29 23:58:00    5.731    4.901   22.365    52.195   74.560  0.187808   \n",
      "2024-02-29 23:59:00    3.925    3.865   86.585   217.102  303.687  0.601014   \n",
      "\n",
      "                           X2        X3        X4        X5  ...      X772  \\\n",
      "2023-03-01 00:00:00 -0.637860  0.006652  0.136870  0.116698  ...  0.333753   \n",
      "2023-03-01 00:01:00 -0.075619  0.431594  0.522400  0.475255  ...  0.333657   \n",
      "2023-03-01 00:02:00 -0.444684  0.100695  0.224729  0.203282  ...  0.333667   \n",
      "2023-03-01 00:03:00 -0.666728 -0.123858  0.019197  0.014459  ...  0.333174   \n",
      "2023-03-01 00:04:00 -0.325973  0.116336  0.234311  0.214073  ...  0.333171   \n",
      "...                       ...       ...       ...       ...  ...       ...   \n",
      "2024-02-29 23:55:00  0.117044  0.277143  0.313719  0.302920  ... -0.124702   \n",
      "2024-02-29 23:56:00  0.095603  0.259092  0.300265  0.292710  ... -0.124794   \n",
      "2024-02-29 23:57:00  0.167857  0.318979  0.357586  0.347999  ... -0.124974   \n",
      "2024-02-29 23:58:00  0.373033  0.494545  0.521237  0.502286  ... -0.124703   \n",
      "2024-02-29 23:59:00  1.016377  1.049614  1.033506  0.981004  ... -0.124313   \n",
      "\n",
      "                         X773      X774      X775      X776      X777  \\\n",
      "2023-03-01 00:00:00 -0.009992 -0.695595 -0.444077 -0.191238 -0.184251   \n",
      "2023-03-01 00:01:00 -0.010040 -0.696226 -0.452866 -0.200082 -0.188929   \n",
      "2023-03-01 00:02:00 -0.010037 -0.696832 -0.461383 -0.208786 -0.193571   \n",
      "2023-03-01 00:03:00 -0.010279 -0.697391 -0.469628 -0.217350 -0.198175   \n",
      "2023-03-01 00:04:00 -0.010283 -0.697940 -0.477622 -0.225780 -0.202745   \n",
      "...                       ...       ...       ...       ...       ...   \n",
      "2024-02-29 23:55:00 -0.076556 -0.691375 -0.395531 -0.144590 -0.160081   \n",
      "2024-02-29 23:56:00 -0.076648 -0.692430 -0.405900 -0.154205 -0.164962   \n",
      "2024-02-29 23:57:00 -0.076833 -0.693393 -0.415936 -0.163664 -0.169803   \n",
      "2024-02-29 23:58:00 -0.076543 -0.694285 -0.425654 -0.172971 -0.174606   \n",
      "2024-02-29 23:59:00 -0.076128 -0.695039 -0.435039 -0.182115 -0.179363   \n",
      "\n",
      "                         X778      X779      X780     label  \n",
      "2023-03-01 00:00:00 -0.471897 -0.625428 -0.553991  0.562539  \n",
      "2023-03-01 00:01:00 -0.472842 -0.625832 -0.554426  0.533686  \n",
      "2023-03-01 00:02:00 -0.473785 -0.626236 -0.554860  0.546505  \n",
      "2023-03-01 00:03:00 -0.474726 -0.626639 -0.555294  0.357703  \n",
      "2023-03-01 00:04:00 -0.475666 -0.627043 -0.555728  0.362452  \n",
      "...                       ...       ...       ...       ...  \n",
      "2024-02-29 23:55:00 -0.467827 -0.667111 -0.893893  0.396289  \n",
      "2024-02-29 23:56:00 -0.468785 -0.667457 -0.894092  0.328993  \n",
      "2024-02-29 23:57:00 -0.469740 -0.667804 -0.894291  0.189909  \n",
      "2024-02-29 23:58:00 -0.470693 -0.668150 -0.894490  0.410831  \n",
      "2024-02-29 23:59:00 -0.471641 -0.668494 -0.894687  0.731542  \n",
      "\n",
      "[525886 rows x 786 columns]>\n",
      "Total features: 785\n",
      "['bid_qty', 'ask_qty', 'buy_qty', 'sell_qty', 'volume', 'X1', 'X2', 'X3', 'X4', 'X5', 'X6', 'X7', 'X8', 'X9', 'X10', 'X11', 'X12', 'X13', 'X14', 'X15', 'X16', 'X17', 'X18', 'X19', 'X20', 'X21', 'X22', 'X23', 'X24', 'X25', 'X26', 'X27', 'X28', 'X29', 'X30', 'X31', 'X32', 'X33', 'X34', 'X35', 'X36', 'X37', 'X38', 'X39', 'X40', 'X41', 'X42', 'X43', 'X44', 'X45', 'X46', 'X47', 'X48', 'X49', 'X50', 'X51', 'X52', 'X53', 'X54', 'X55', 'X56', 'X57', 'X58', 'X59', 'X60', 'X61', 'X62', 'X63', 'X64', 'X65', 'X66', 'X67', 'X68', 'X69', 'X70', 'X71', 'X72', 'X73', 'X74', 'X75', 'X76', 'X77', 'X78', 'X79', 'X80', 'X81', 'X82', 'X83', 'X84', 'X85', 'X86', 'X87', 'X88', 'X89', 'X90', 'X91', 'X92', 'X93', 'X94', 'X95', 'X96', 'X97', 'X98', 'X99', 'X100', 'X101', 'X102', 'X103', 'X104', 'X105', 'X106', 'X107', 'X108', 'X109', 'X110', 'X111', 'X112', 'X113', 'X114', 'X115', 'X116', 'X117', 'X118', 'X119', 'X120', 'X121', 'X122', 'X123', 'X124', 'X125', 'X126', 'X127', 'X128', 'X129', 'X130', 'X131', 'X132', 'X133', 'X134', 'X135', 'X136', 'X137', 'X138', 'X139', 'X140', 'X141', 'X142', 'X143', 'X144', 'X145', 'X146', 'X147', 'X148', 'X149', 'X150', 'X151', 'X152', 'X153', 'X154', 'X155', 'X156', 'X157', 'X158', 'X159', 'X160', 'X161', 'X162', 'X163', 'X164', 'X165', 'X166', 'X167', 'X168', 'X169', 'X170', 'X171', 'X172', 'X173', 'X174', 'X175', 'X176', 'X177', 'X178', 'X179', 'X180', 'X181', 'X182', 'X183', 'X184', 'X185', 'X186', 'X187', 'X188', 'X189', 'X190', 'X191', 'X192', 'X193', 'X194', 'X195', 'X196', 'X197', 'X198', 'X199', 'X200', 'X201', 'X202', 'X203', 'X204', 'X205', 'X206', 'X207', 'X208', 'X209', 'X210', 'X211', 'X212', 'X213', 'X214', 'X215', 'X216', 'X217', 'X218', 'X219', 'X220', 'X221', 'X222', 'X223', 'X224', 'X225', 'X226', 'X227', 'X228', 'X229', 'X230', 'X231', 'X232', 'X233', 'X234', 'X235', 'X236', 'X237', 'X238', 'X239', 'X240', 'X241', 'X242', 'X243', 'X244', 'X245', 'X246', 'X247', 'X248', 'X249', 'X250', 'X251', 'X252', 'X253', 'X254', 'X255', 'X256', 'X257', 'X258', 'X259', 'X260', 'X261', 'X262', 'X263', 'X264', 'X265', 'X266', 'X267', 'X268', 'X269', 'X270', 'X271', 'X272', 'X273', 'X274', 'X275', 'X276', 'X277', 'X278', 'X279', 'X280', 'X281', 'X282', 'X283', 'X284', 'X285', 'X286', 'X287', 'X288', 'X289', 'X290', 'X291', 'X292', 'X293', 'X294', 'X295', 'X296', 'X297', 'X298', 'X299', 'X300', 'X301', 'X302', 'X303', 'X304', 'X305', 'X306', 'X307', 'X308', 'X309', 'X310', 'X311', 'X312', 'X313', 'X314', 'X315', 'X316', 'X317', 'X318', 'X319', 'X320', 'X321', 'X322', 'X323', 'X324', 'X325', 'X326', 'X327', 'X328', 'X329', 'X330', 'X331', 'X332', 'X333', 'X334', 'X335', 'X336', 'X337', 'X338', 'X339', 'X340', 'X341', 'X342', 'X343', 'X344', 'X345', 'X346', 'X347', 'X348', 'X349', 'X350', 'X351', 'X352', 'X353', 'X354', 'X355', 'X356', 'X357', 'X358', 'X359', 'X360', 'X361', 'X362', 'X363', 'X364', 'X365', 'X366', 'X367', 'X368', 'X369', 'X370', 'X371', 'X372', 'X373', 'X374', 'X375', 'X376', 'X377', 'X378', 'X379', 'X380', 'X381', 'X382', 'X383', 'X384', 'X385', 'X386', 'X387', 'X388', 'X389', 'X390', 'X391', 'X392', 'X393', 'X394', 'X395', 'X396', 'X397', 'X398', 'X399', 'X400', 'X401', 'X402', 'X403', 'X404', 'X405', 'X406', 'X407', 'X408', 'X409', 'X410', 'X411', 'X412', 'X413', 'X414', 'X415', 'X416', 'X417', 'X418', 'X419', 'X420', 'X421', 'X422', 'X423', 'X424', 'X425', 'X426', 'X427', 'X428', 'X429', 'X430', 'X431', 'X432', 'X433', 'X434', 'X435', 'X436', 'X437', 'X438', 'X439', 'X440', 'X441', 'X442', 'X443', 'X444', 'X445', 'X446', 'X447', 'X448', 'X449', 'X450', 'X451', 'X452', 'X453', 'X454', 'X455', 'X456', 'X457', 'X458', 'X459', 'X460', 'X461', 'X462', 'X463', 'X464', 'X465', 'X466', 'X467', 'X468', 'X469', 'X470', 'X471', 'X472', 'X473', 'X474', 'X475', 'X476', 'X477', 'X478', 'X479', 'X480', 'X481', 'X482', 'X483', 'X484', 'X485', 'X486', 'X487', 'X488', 'X489', 'X490', 'X491', 'X492', 'X493', 'X494', 'X495', 'X496', 'X497', 'X498', 'X499', 'X500', 'X501', 'X502', 'X503', 'X504', 'X505', 'X506', 'X507', 'X508', 'X509', 'X510', 'X511', 'X512', 'X513', 'X514', 'X515', 'X516', 'X517', 'X518', 'X519', 'X520', 'X521', 'X522', 'X523', 'X524', 'X525', 'X526', 'X527', 'X528', 'X529', 'X530', 'X531', 'X532', 'X533', 'X534', 'X535', 'X536', 'X537', 'X538', 'X539', 'X540', 'X541', 'X542', 'X543', 'X544', 'X545', 'X546', 'X547', 'X548', 'X549', 'X550', 'X551', 'X552', 'X553', 'X554', 'X555', 'X556', 'X557', 'X558', 'X559', 'X560', 'X561', 'X562', 'X563', 'X564', 'X565', 'X566', 'X567', 'X568', 'X569', 'X570', 'X571', 'X572', 'X573', 'X574', 'X575', 'X576', 'X577', 'X578', 'X579', 'X580', 'X581', 'X582', 'X583', 'X584', 'X585', 'X586', 'X587', 'X588', 'X589', 'X590', 'X591', 'X592', 'X593', 'X594', 'X595', 'X596', 'X597', 'X598', 'X599', 'X600', 'X601', 'X602', 'X603', 'X604', 'X605', 'X606', 'X607', 'X608', 'X609', 'X610', 'X611', 'X612', 'X613', 'X614', 'X615', 'X616', 'X617', 'X618', 'X619', 'X620', 'X621', 'X622', 'X623', 'X624', 'X625', 'X626', 'X627', 'X628', 'X629', 'X630', 'X631', 'X632', 'X633', 'X634', 'X635', 'X636', 'X637', 'X638', 'X639', 'X640', 'X641', 'X642', 'X643', 'X644', 'X645', 'X646', 'X647', 'X648', 'X649', 'X650', 'X651', 'X652', 'X653', 'X654', 'X655', 'X656', 'X657', 'X658', 'X659', 'X660', 'X661', 'X662', 'X663', 'X664', 'X665', 'X666', 'X667', 'X668', 'X669', 'X670', 'X671', 'X672', 'X673', 'X674', 'X675', 'X676', 'X677', 'X678', 'X679', 'X680', 'X681', 'X682', 'X683', 'X684', 'X685', 'X686', 'X687', 'X688', 'X689', 'X690', 'X691', 'X692', 'X693', 'X694', 'X695', 'X696', 'X697', 'X698', 'X699', 'X700', 'X701', 'X702', 'X703', 'X704', 'X705', 'X706', 'X707', 'X708', 'X709', 'X710', 'X711', 'X712', 'X713', 'X714', 'X715', 'X716', 'X717', 'X718', 'X719', 'X720', 'X721', 'X722', 'X723', 'X724', 'X725', 'X726', 'X727', 'X728', 'X729', 'X730', 'X731', 'X732', 'X733', 'X734', 'X735', 'X736', 'X737', 'X738', 'X739', 'X740', 'X741', 'X742', 'X743', 'X744', 'X745', 'X746', 'X747', 'X748', 'X749', 'X750', 'X751', 'X752', 'X753', 'X754', 'X755', 'X756', 'X757', 'X758', 'X759', 'X760', 'X761', 'X762', 'X763', 'X764', 'X765', 'X766', 'X767', 'X768', 'X769', 'X770', 'X771', 'X772', 'X773', 'X774', 'X775', 'X776', 'X777', 'X778', 'X779', 'X780']\n"
     ]
    }
   ],
   "source": [
    "# ==============================\n",
    "# Data Handling\n",
    "# ==============================\n",
    "\n",
    "# Load train data:\n",
    "df_train = pd.read_parquet('train.parquet', engine='pyarrow')\n",
    "print(df_train.head)\n",
    "\n",
    "# Get list of all feature columns\n",
    "feature_columns = [col for col in df_train.columns if col != 'label']  # Exclude target if present\n",
    "print(f\"Total features: {len(feature_columns)}\")\n",
    "print(feature_columns)\n",
    "\n",
    "# Load test data:\n",
    "df_competition = pd.read_parquet('test.parquet', engine='pyarrow')\n",
    "print(df_competition.head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c721681",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and preparing data for feature selection...\n",
      "Feature selection data shape: (525887, 895)\n",
      "Starting feature selection: 895 -> 200 features\n",
      "  Removing highly correlated features...\n",
      "    Removed 275 highly correlated features\n",
      "After correlation filtering: 620 features\n",
      "Calculating feature importance metrics...\n",
      "  Computing mutual information...\n",
      "  Computing correlations...\n",
      "  Computing Spearman correlations...\n",
      "  Computing F-statistics...\n",
      "  Computing Lasso coefficients...\n",
      "  Computing feature stability...\n",
      "Final selection: 200 features\n",
      "\n",
      "Top 10 features:\n",
      "  1. X21: 0.2466\n",
      "  2. X20: 0.2378\n",
      "  3. X28: 0.2179\n",
      "  4. X863: 0.2142\n",
      "  5. X19: 0.2099\n",
      "  6. X27: 0.2094\n",
      "  7. X29: 0.2060\n",
      "  8. X858: 0.1949\n",
      "  9. X860: 0.1913\n",
      "  10. X219: 0.1830\n",
      "\n",
      "✅ Feature selection completed!\n",
      "Selected 200 features out of 895\n",
      "Memory usage reduced by ~77.7%\n"
     ]
    }
   ],
   "source": [
    "# ==============================\n",
    "# Feature Selection for Crypto Trading Data\n",
    "# ==============================\n",
    "\n",
    "import gc\n",
    "from sklearn.feature_selection import mutual_info_regression, SelectKBest, f_regression\n",
    "from sklearn.linear_model import LassoCV\n",
    "from scipy.stats import spearmanr\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "NUM_FEATURES = 200  # Target number of features to keep\n",
    "\n",
    "def calculate_feature_importance_metrics(X, y, feature_names):\n",
    "    \"\"\"\n",
    "    Calculate multiple feature importance metrics for crypto trading data\n",
    "    \"\"\"\n",
    "    print(\"Calculating feature importance metrics...\")\n",
    "    results = {}\n",
    "    \n",
    "    # 1. MUTUAL INFORMATION (Best for non-linear relationships)\n",
    "    print(\"  Computing mutual information...\")\n",
    "    mi_scores = mutual_info_regression(X, y, random_state=42, n_neighbors=5)\n",
    "    results['mutual_info'] = dict(zip(feature_names, mi_scores))\n",
    "    \n",
    "    # 2. CORRELATION WITH TARGET (Linear relationships)\n",
    "    print(\"  Computing correlations...\")\n",
    "    correlations = []\n",
    "    for i, col in enumerate(feature_names):\n",
    "        try:\n",
    "            corr = np.corrcoef(X[:, i], y)[0, 1]\n",
    "            correlations.append(abs(corr) if not np.isnan(corr) else 0)\n",
    "        except:\n",
    "            correlations.append(0)\n",
    "    results['correlation'] = dict(zip(feature_names, correlations))\n",
    "    \n",
    "    # 3. SPEARMAN CORRELATION (Monotonic relationships)\n",
    "    print(\"  Computing Spearman correlations...\")\n",
    "    spearman_corrs = []\n",
    "    for i, col in enumerate(feature_names):\n",
    "        try:\n",
    "            corr, _ = spearmanr(X[:, i], y)\n",
    "            spearman_corrs.append(abs(corr) if not np.isnan(corr) else 0)\n",
    "        except:\n",
    "            spearman_corrs.append(0)\n",
    "    results['spearman'] = dict(zip(feature_names, spearman_corrs))\n",
    "    \n",
    "    # 4. F-STATISTIC (Linear model relevance)\n",
    "    print(\"  Computing F-statistics...\")\n",
    "    f_scores, _ = f_regression(X, y)\n",
    "    results['f_statistic'] = dict(zip(feature_names, f_scores))\n",
    "    \n",
    "    # 5. LASSO REGULARIZATION (Sparse selection)\n",
    "    print(\"  Computing Lasso coefficients...\")\n",
    "    try:\n",
    "        # Use a subset for Lasso to avoid memory issues\n",
    "        sample_size = min(10000, len(X))\n",
    "        idx = np.random.choice(len(X), sample_size, replace=False)\n",
    "        lasso = LassoCV(cv=3, random_state=42, max_iter=1000)\n",
    "        lasso.fit(X[idx], y[idx])\n",
    "        lasso_coefs = abs(lasso.coef_)\n",
    "        results['lasso'] = dict(zip(feature_names, lasso_coefs))\n",
    "    except Exception as e:\n",
    "        print(f\"    Lasso failed: {e}\")\n",
    "        results['lasso'] = dict(zip(feature_names, [0] * len(feature_names)))\n",
    "    \n",
    "    return results\n",
    "\n",
    "def calculate_feature_stability(X, feature_names, window_size=1000):\n",
    "    \"\"\"\n",
    "    Calculate feature stability over time (important for crypto)\n",
    "    \"\"\"\n",
    "    print(\"  Computing feature stability...\")\n",
    "    stabilities = []\n",
    "    \n",
    "    for i, col in enumerate(feature_names):\n",
    "        feature_data = X[:, i]\n",
    "        \n",
    "        # Calculate rolling standard deviation\n",
    "        if len(feature_data) > window_size:\n",
    "            rolling_stds = []\n",
    "            for j in range(0, len(feature_data) - window_size, window_size // 2):\n",
    "                window_data = feature_data[j:j + window_size]\n",
    "                rolling_stds.append(np.std(window_data))\n",
    "            \n",
    "            # Stability = inverse of variance in rolling std\n",
    "            stability = 1 / (1 + np.var(rolling_stds)) if len(rolling_stds) > 1 else 0\n",
    "        else:\n",
    "            stability = 0\n",
    "            \n",
    "        stabilities.append(stability)\n",
    "    \n",
    "    return dict(zip(feature_names, stabilities))\n",
    "\n",
    "def remove_highly_correlated_features(X, feature_names, threshold=0.95):\n",
    "    \"\"\"\n",
    "    Remove features that are highly correlated with each other\n",
    "    \"\"\"\n",
    "    print(\"  Removing highly correlated features...\")\n",
    "    \n",
    "    # Calculate correlation matrix\n",
    "    corr_matrix = np.corrcoef(X.T)\n",
    "    \n",
    "    # Find highly correlated pairs\n",
    "    to_remove = set()\n",
    "    for i in range(len(corr_matrix)):\n",
    "        for j in range(i + 1, len(corr_matrix)):\n",
    "            if abs(corr_matrix[i, j]) > threshold:\n",
    "                # Remove the feature with lower variance\n",
    "                var_i = np.var(X[:, i])\n",
    "                var_j = np.var(X[:, j])\n",
    "                to_remove.add(j if var_i > var_j else i)\n",
    "    \n",
    "    # Keep features not in removal set\n",
    "    keep_indices = [i for i in range(len(feature_names)) if i not in to_remove]\n",
    "    \n",
    "    print(f\"    Removed {len(to_remove)} highly correlated features\")\n",
    "    return keep_indices\n",
    "\n",
    "def select_best_features(X, y, feature_names, target_count=200):\n",
    "    \"\"\"\n",
    "    Comprehensive feature selection for crypto trading\n",
    "    \"\"\"\n",
    "    print(f\"Starting feature selection: {len(feature_names)} -> {target_count} features\")\n",
    "\n",
    "    # clean data:\n",
    "\n",
    "    \n",
    "    \n",
    "    # Step 1: Remove highly correlated features\n",
    "    keep_indices = remove_highly_correlated_features(X, feature_names, threshold=0.95)\n",
    "    X_filtered = X[:, keep_indices]\n",
    "    feature_names_filtered = [feature_names[i] for i in keep_indices]\n",
    "    \n",
    "    print(f\"After correlation filtering: {len(feature_names_filtered)} features\")\n",
    "    \n",
    "    # Step 2: Calculate importance metrics\n",
    "    importance_metrics = calculate_feature_importance_metrics(X_filtered, y, feature_names_filtered)\n",
    "    \n",
    "    # Step 3: Calculate stability\n",
    "    stability_scores = calculate_feature_stability(X_filtered, feature_names_filtered)\n",
    "    \n",
    "    # Step 4: Combine metrics with weights optimized for crypto trading\n",
    "    feature_scores = {}\n",
    "    for feature in feature_names_filtered:\n",
    "        # Weighted combination - prioritize metrics that work well for crypto\n",
    "        score = (\n",
    "            0.30 * importance_metrics['mutual_info'][feature] +      # Non-linear patterns\n",
    "            0.25 * importance_metrics['spearman'][feature] +         # Monotonic relationships  \n",
    "            0.20 * importance_metrics['correlation'][feature] +      # Linear relationships\n",
    "            0.15 * (importance_metrics['f_statistic'][feature] / max(importance_metrics['f_statistic'].values())) +  # Normalized F-stat\n",
    "            0.05 * importance_metrics['lasso'][feature] +            # Sparse selection\n",
    "            0.05 * stability_scores[feature]                         # Temporal stability\n",
    "        )\n",
    "        feature_scores[feature] = score\n",
    "    \n",
    "    # Step 5: Select top features\n",
    "    top_features = sorted(feature_scores.items(), key=lambda x: x[1], reverse=True)[:target_count]\n",
    "    selected_features = [feat[0] for feat in top_features]\n",
    "    \n",
    "    print(f\"Final selection: {len(selected_features)} features\")\n",
    "    \n",
    "    # Print top 10 features for inspection\n",
    "    print(\"\\nTop 10 features:\")\n",
    "    for i, (feat, score) in enumerate(top_features[:10]):\n",
    "        print(f\"  {i+1}. {feat}: {score:.4f}\")\n",
    "    \n",
    "    return selected_features, feature_scores\n",
    "\n",
    "# Memory-efficient feature selection\n",
    "print(\"Loading and preparing data for feature selection...\")\n",
    "\n",
    "df_sample = df_train.copy()\n",
    "\n",
    "# Replace infinities with NaN\n",
    "df_sample.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "# Forward fill (appropriate for time series)\n",
    "df_sample.fillna(method='ffill', inplace=True)\n",
    "\n",
    "# Fill remaining NaNs with 0\n",
    "df_sample.fillna(0, inplace=True)\n",
    "\n",
    "# Prepare features and target\n",
    "feature_columns = [col for col in df_sample.columns if col != 'label']\n",
    "X_sample = df_sample[feature_columns].fillna(0).values  # Simple fillna for selection\n",
    "y_sample = df_sample['label'].values\n",
    "\n",
    "\n",
    "print(f\"Feature selection data shape: {X_sample.shape}\")\n",
    "\n",
    "# Run feature selection\n",
    "selected_features, feature_importance_scores = select_best_features(\n",
    "    X_sample, y_sample, feature_columns, target_count=NUM_FEATURES\n",
    ")\n",
    "\n",
    "# Clean up memory\n",
    "del X_sample, y_sample, df_sample\n",
    "gc.collect()\n",
    "\n",
    "print(f\"\\n✅ Feature selection completed!\")\n",
    "print(f\"Selected {len(selected_features)} features out of {len(feature_columns)}\")\n",
    "print(f\"Memory usage reduced by ~{(1 - len(selected_features)/len(feature_columns))*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d7a6e43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying feature selection to train and test datasets...\n",
      "Processing training data...\n",
      "Processing test data...\n",
      "Handling missing values...\n",
      "Final training data shape: (525887, 201)\n",
      "Final test data shape: (538150, 200)\n",
      "✅ Data preprocessing completed!\n"
     ]
    }
   ],
   "source": [
    "# ==============================\n",
    "# Apply Feature Selection to Datasets\n",
    "# ==============================\n",
    "\n",
    "print(\"Applying feature selection to train and test datasets...\")\n",
    "\n",
    "# Apply to training data\n",
    "print(\"Processing training data...\")\n",
    "df_train_filtered = df_train[['label'] + selected_features].copy()\n",
    "\n",
    "# Apply to test data  \n",
    "print(\"Processing test data...\")\n",
    "df_test_filtered = df_competition[selected_features].copy()\n",
    "\n",
    "# Handle missing values properly\n",
    "print(\"Handling missing values...\")\n",
    "for df in [df_train_filtered, df_test_filtered]:\n",
    "    # Replace infinities with NaN\n",
    "    df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    \n",
    "    # Forward fill (appropriate for time series)\n",
    "    df.fillna(method='ffill', inplace=True)\n",
    "    \n",
    "    # Fill remaining NaNs with 0\n",
    "    df.fillna(0, inplace=True)\n",
    "\n",
    "print(f\"Final training data shape: {df_train_filtered.shape}\")\n",
    "print(f\"Final test data shape: {df_test_filtered.shape}\")\n",
    "gc.collect()\n",
    "\n",
    "print(\"✅ Data preprocessing completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c51c1d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# Ensemble Model\n",
    "# ==============================\n",
    "\n",
    "\"\"\"\n",
    "- Trees: LightGBM, CatBoost\n",
    "- Neural Network: TabNet\n",
    "- Time Series: N-HiTS\n",
    "- Linear: Ridge\n",
    "\n",
    "\"\"\"\n",
    "models_config = {\n",
    "    'lightgbm': {\n",
    "        'weight': 0.40,\n",
    "        'params': {\n",
    "            'objective': 'regression',\n",
    "            'metric': 'rmse',\n",
    "            'boosting_type': 'gbdt',\n",
    "            'num_leaves': 10,  # Very conservative\n",
    "            'learning_rate': 0.005,  # Very slow learning\n",
    "            'feature_fraction': 0.6,  # Use fewer features\n",
    "            'bagging_fraction': 0.6,  # More aggressive bagging\n",
    "            'bagging_freq': 5,\n",
    "            'min_data_in_leaf': 50,  # Require more data per leaf\n",
    "            'lambda_l1': 0.5,  # Strong L1 regularization\n",
    "            'lambda_l2': 0.5,  # Strong L2 regularization\n",
    "            'verbose': -1,\n",
    "            'random_state': 42\n",
    "        }\n",
    "    },\n",
    "    \n",
    "    'catboost': {\n",
    "        'weight': 0.30,\n",
    "        'params': {\n",
    "            'iterations': 200,  # Fewer iterations\n",
    "            'learning_rate': 0.005,  # Very slow learning\n",
    "            'depth': 3,  # Very shallow trees\n",
    "            'l2_leaf_reg': 10,  # Strong regularization\n",
    "            'loss_function': 'RMSE',\n",
    "            'eval_metric': 'RMSE',\n",
    "            'random_seed': 42,\n",
    "            'verbose': False,\n",
    "            'min_data_in_leaf': 50  # Require more data\n",
    "        }\n",
    "    },\n",
    "    \n",
    "    'ridge': {\n",
    "        'weight': 0.20,\n",
    "        'params': {\n",
    "            'alpha': 50.0,  # Very strong regularization\n",
    "            'random_state': 42\n",
    "        }\n",
    "    },\n",
    "    \n",
    "    # Correct TabNet configuration\n",
    "    'tabnet': {\n",
    "    'weight': 0.15,\n",
    "    'params': {\n",
    "        'optimizer_fn': torch.optim.Adam,\n",
    "        'optimizer_params': {'lr': 2e-2},\n",
    "        'scheduler_params': {'step_size': 50, 'gamma': 0.9},\n",
    "        'scheduler_fn': torch.optim.lr_scheduler.StepLR,\n",
    "        'mask_type': 'entmax',\n",
    "        'n_d': 8,\n",
    "        'n_a': 8,\n",
    "        'n_steps': 3,\n",
    "        'gamma': 1.3,\n",
    "        'n_independent': 2,\n",
    "        'n_shared': 2,\n",
    "        'lambda_sparse': 1e-4,\n",
    "        'momentum': 0.3,\n",
    "        'clip_value': 2,\n",
    "        'verbose': 10\n",
    "        }\n",
    "    },\n",
    "    \n",
    "    'extra_trees': {\n",
    "        'weight': 0.10,\n",
    "        'params': {\n",
    "            'n_estimators': 25,  # Fewer trees\n",
    "            'max_depth': 4,  # Very shallow\n",
    "            'min_samples_split': 20,  # Require more data to split\n",
    "            'min_samples_leaf': 10,  # Require more data per leaf\n",
    "            'random_state': 42\n",
    "        }\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec556d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CompetitiveEnsemble:\n",
    "    def __init__(self, models_config):\n",
    "        self.models_config = models_config\n",
    "        self.models = {}\n",
    "        self.weights = {}\n",
    "        self.is_fitted = False\n",
    "        \n",
    "    def train_model(self, model_name, config, X_train, y_train, X_val=None, y_val=None):\n",
    "        \"\"\"Train individual model\"\"\"\n",
    "        \n",
    "        if model_name == 'lightgbm':\n",
    "            train_data = lgb.Dataset(X_train, label=y_train)\n",
    "            if X_val is not None:\n",
    "                val_data = lgb.Dataset(X_val, label=y_val, reference=train_data)\n",
    "                model = lgb.train(\n",
    "                    config['params'],\n",
    "                    train_data,\n",
    "                    valid_sets=[val_data],\n",
    "                    callbacks=[lgb.early_stopping(50), lgb.log_evaluation(0)]\n",
    "                )\n",
    "            else:\n",
    "                model = lgb.train(config['params'], train_data)\n",
    "                \n",
    "        elif model_name == 'catboost':\n",
    "            if X_val is not None:\n",
    "                model = cb.CatBoostRegressor(**config['params'])\n",
    "                model.fit(X_train, y_train, eval_set=(X_val, y_val), early_stopping_rounds=50, verbose=False)\n",
    "            else:\n",
    "                model = cb.CatBoostRegressor(**config['params'])\n",
    "                model.fit(X_train, y_train, verbose=False)\n",
    "                \n",
    "        elif model_name == 'ridge':\n",
    "            model = Ridge(**config['params'])\n",
    "            model.fit(X_train, y_train)\n",
    "            \n",
    "        elif model_name == 'tabnet':\n",
    "            model = TabNetRegressor(**config['params'])\n",
    "            # Reshape y_train and y_val to 2D\n",
    "            y_train_2d = y_train.reshape(-1, 1)\n",
    "            y_val_2d = y_val.reshape(-1, 1) if y_val is not None else None\n",
    "            \n",
    "            model.fit(\n",
    "                X_train, y_train_2d,  # Use 2D target\n",
    "                eval_set=[(X_val, y_val_2d)] if y_val_2d is not None else None,\n",
    "                max_epochs=50,\n",
    "                patience=20,\n",
    "                batch_size=256,\n",
    "                virtual_batch_size=128,\n",
    "                num_workers=0,\n",
    "                drop_last=False\n",
    "            )\n",
    "            \n",
    "        elif model_name == 'extra_trees':\n",
    "            from sklearn.ensemble import ExtraTreesRegressor\n",
    "            model = ExtraTreesRegressor(**config['params'])\n",
    "            model.fit(X_train, y_train)\n",
    "            \n",
    "        return model\n",
    "    \n",
    "    def fit(self, X, y, validation_split=0.2):\n",
    "        \"\"\"Train all models and determine optimal weights\"\"\"\n",
    "        print(\"Training competitive ensemble...\")\n",
    "        \n",
    "        # Split data for validation\n",
    "        split_idx = int(len(X) * (1 - validation_split))\n",
    "        X_train, X_val = X[:split_idx], X[split_idx:]\n",
    "        y_train, y_val = y[:split_idx], y[split_idx:]\n",
    "        \n",
    "        # Train all models\n",
    "        for model_name, config in self.models_config.items():\n",
    "            print(f\"Training {model_name}...\")\n",
    "            try:\n",
    "                self.models[model_name] = self.train_model(\n",
    "                    model_name, config, X_train, y_train, X_val, y_val\n",
    "                )\n",
    "            except Exception as e:\n",
    "                print(f\"Error training {model_name}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        # Get validation predictions\n",
    "        val_predictions = {}\n",
    "        for model_name, model in self.models.items():\n",
    "            try:\n",
    "                preds = model.predict(X_val)\n",
    "                val_predictions[model_name] = preds\n",
    "            except Exception as e:\n",
    "                print(f\"Error predicting with {model_name}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        # Calculate adaptive weights\n",
    "        base_weights = {name: config['weight'] for name, config in self.models_config.items() \n",
    "                       if name in self.models}\n",
    "        \n",
    "        self.weights = self.calculate_adaptive_weights(val_predictions, y_val, base_weights)\n",
    "        \n",
    "        # Print model performance\n",
    "        print(\"\\n=== MODEL PERFORMANCE ===\")\n",
    "        for model_name, preds in val_predictions.items():\n",
    "            # Ensure both are 1D for correlation calculation\n",
    "            preds_1d = preds.ravel() if preds.ndim > 1 else preds\n",
    "            actual_1d = y_val.ravel() if y_val.ndim > 1 else y_val\n",
    "            \n",
    "            corr = np.corrcoef(actual_1d, preds_1d)[0, 1]\n",
    "            rmse = np.sqrt(mean_squared_error(actual_1d, preds_1d))\n",
    "            weight = self.weights[model_name]\n",
    "            print(f\"{model_name}: Corr={corr:.4f}, RMSE={rmse:.4f}, Weight={weight:.3f}\")\n",
    "        \n",
    "        self.is_fitted = True\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Make ensemble prediction\"\"\"\n",
    "        if not self.is_fitted:\n",
    "            raise ValueError(\"Model must be fitted before prediction\")\n",
    "        \n",
    "        predictions = {}\n",
    "        for model_name, model in self.models.items():\n",
    "            try:\n",
    "                preds = model.predict(X)\n",
    "                # Ensure predictions are 1D\n",
    "                preds_1d = preds.ravel() if preds.ndim > 1 else preds\n",
    "                predictions[model_name] = preds_1d\n",
    "            except Exception as e:\n",
    "                print(f\"Error predicting with {model_name}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        # Weighted ensemble\n",
    "        final_prediction = np.zeros(len(X))\n",
    "        for model_name, preds in predictions.items():\n",
    "            final_prediction += self.weights[model_name] * preds\n",
    "        \n",
    "        return final_prediction\n",
    "    \n",
    "    def calculate_adaptive_weights(self, predictions, actual, base_weights):\n",
    "        \"\"\"Calculate adaptive weights based on validation performance\"\"\"\n",
    "        # Calculate performance metrics\n",
    "        performance_scores = {}\n",
    "        \n",
    "        for model_name, preds in predictions.items():\n",
    "            # Ensure predictions and actual are 1D for correlation calculation\n",
    "            preds_1d = preds.ravel() if preds.ndim > 1 else preds\n",
    "            actual_1d = actual.ravel() if actual.ndim > 1 else actual\n",
    "            \n",
    "            # Use Pearson correlation as primary metric (competition metric)\n",
    "            corr = np.corrcoef(actual_1d, preds_1d)[0, 1]\n",
    "            if np.isnan(corr):\n",
    "                corr = 0\n",
    "            performance_scores[model_name] = max(0, corr)  # Ensure non-negative\n",
    "        \n",
    "        # Normalize performance scores\n",
    "        total_performance = sum(performance_scores.values())\n",
    "        if total_performance > 0:\n",
    "            performance_weights = {k: v/total_performance for k, v in performance_scores.items()}\n",
    "        else:\n",
    "            performance_weights = base_weights\n",
    "        \n",
    "        # Blend base weights with performance weights\n",
    "        alpha = 0.7  # Weight for base weights vs performance\n",
    "        final_weights = {}\n",
    "        for model_name in base_weights.keys():\n",
    "            final_weights[model_name] = (\n",
    "                alpha * base_weights[model_name] + \n",
    "                (1 - alpha) * performance_weights[model_name]\n",
    "            )\n",
    "        \n",
    "        # Renormalize\n",
    "        total_weight = sum(final_weights.values())\n",
    "        final_weights = {k: v/total_weight for k, v in final_weights.items()}\n",
    "        \n",
    "        return final_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf20bb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (525887, 200)\n",
      "Test data shape: (538150, 200)\n",
      "Training competitive ensemble...\n",
      "Training lightgbm...\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[4]\tvalid_0's rmse: 1.04052\n",
      "Training catboost...\n",
      "Training ridge...\n",
      "Training tabnet...\n",
      "epoch 0  | loss: 0.99065 | val_0_mse: 1.14846 |  0:00:51s\n",
      "epoch 10 | loss: 0.37332 | val_0_mse: 2.89719 |  0:09:27s\n",
      "epoch 20 | loss: 0.30245 | val_0_mse: 2.22098 |  0:18:04s\n",
      "\n",
      "Early stopping occurred at epoch 20 with best_epoch = 0 and best_val_0_mse = 1.14846\n",
      "Training extra_trees...\n",
      "\n",
      "=== MODEL PERFORMANCE ===\n",
      "lightgbm: Corr=0.0164, RMSE=1.0405, Weight=0.280\n",
      "catboost: Corr=0.0055, RMSE=1.0405, Weight=0.199\n",
      "ridge: Corr=0.0891, RMSE=1.1470, Weight=0.272\n",
      "tabnet: Corr=0.0305, RMSE=1.0717, Weight=0.145\n",
      "extra_trees: Corr=0.0256, RMSE=1.0692, Weight=0.105\n",
      "\n",
      "=== ENSEMBLE RESULTS ===\n",
      "Predictions shape: (538150,)\n",
      "Prediction range: [-1.7145, 2.4904]\n",
      "Prediction mean: -0.0031\n",
      "Prediction std: 0.1939\n",
      "✅ Submission saved as 'ensemble_submission.csv'\n"
     ]
    }
   ],
   "source": [
    "# ==============================\n",
    "# Train and Predict\n",
    "# ==============================\n",
    "\n",
    "# Prepare data (using your already filtered 2024+ data)\n",
    "X_train = df_train_filtered.drop('label', axis=1).values\n",
    "y_train = df_train_filtered['label'].values\n",
    "X_test = df_test_filtered.values\n",
    "\n",
    "print(f\"Training data shape: {X_train.shape}\")\n",
    "print(f\"Test data shape: {X_test.shape}\")\n",
    "\n",
    "# Initialize and train ensemble\n",
    "ensemble = CompetitiveEnsemble(models_config)\n",
    "ensemble.fit(X_train, y_train, validation_split=0.2)\n",
    "\n",
    "# Make predictions\n",
    "predictions = ensemble.predict(X_test)\n",
    "\n",
    "print(f\"\\n=== ENSEMBLE RESULTS ===\")\n",
    "print(f\"Predictions shape: {predictions.shape}\")\n",
    "print(f\"Prediction range: [{predictions.min():.4f}, {predictions.max():.4f}]\")\n",
    "print(f\"Prediction mean: {predictions.mean():.4f}\")\n",
    "print(f\"Prediction std: {predictions.std():.4f}\")\n",
    "\n",
    "# Save predictions\n",
    "submission = pd.DataFrame({\n",
    "    'ID': df_competition.index,\n",
    "    'prediction': predictions\n",
    "})\n",
    "submission.to_csv('ensemble_submission.csv', index=False)\n",
    "print(\"✅ Submission saved as 'ensemble_submission.csv'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
